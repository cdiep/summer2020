{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors\n",
    "\n",
    "Central to all neural networks in PyTorch is the tensor class. Tensors are similar to NumPyâ€™s ndarrays, with the following addition:\n",
    "* Tensors can also be used on a GPU to accelerate computing.\n",
    "* Tensors can be set to automatically track all operations on them and compute gradients for backprop.\n",
    "* Tensors can be converted to NumPy ndarrays and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# type of tensors\n",
    "torch.zeros(2,3)                  # default dtype float32\n",
    "torch.ones(2,3)                   # default dtype float32\n",
    "torch.empty(2,3)                  # default dtype float32, un-initialized\n",
    "torch.rand(2,3)                   # default dtype float32\n",
    "torch.Tensor(2,3)                 # default float32\n",
    "\n",
    "# construct tensors and initialize them to defined values\n",
    "torch.tensor([[2,3,4],[1,2,3]])   # default dtype int64\n",
    "torch.Tensor([[2,3,4],[1,2,3]])   # default dtype float32, can not set attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor `device` attribute\n",
    "  \n",
    "Tensors can be moved from CPU to GPU devices or vice versa by setting the appropriate values to the `device(torch.device)` attribute."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Moving tensor from cpu to cuda device or vice versa\n",
    "\n",
    "# default setting\n",
    "x = torch.Tensor([[1,2,3],[5,6,7]])\n",
    "print(\"Create a tensor with default settings:\", x.dtype, x.device, x.requires_grad)\n",
    "print(\"Tensor will be created on CPU\")\n",
    "print(x,\"\\n\")\n",
    "\n",
    "# move a tensor on CPU to GPU device\n",
    "device = torch.device(\"cuda\")\n",
    "x = x.to(device)\n",
    "print(\"Move tensor to GPU:\", x.dtype, x.device, x.requires_grad)\n",
    "print(x,\"\\n\")\n",
    "\n",
    "# create tensor on GPU\n",
    "x = torch.tensor([[10,20,30],[50,60,70]], device=device)\n",
    "print(\"Create a tensor on GPU:\", x.dtype, x.device, x.requires_grad)\n",
    "print(x,\"\\n\")\n",
    "\n",
    "# move tensor on GPU to CPU\n",
    "device = torch.device(\"cpu\")\n",
    "x = x.to(device)\n",
    "print(\"Move tensor to CPU:\", x.dtype, x.device, x.requires_grad)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor `requires_grad` attribute\n",
    "\n",
    "The `requires_grad` attribute is used to enable automatic differentiation. Whenever a tensor's  `requires_grad` attribute is set as `True`, it starts to track all operations on it. Afterward, when its `.backward()` function is called, the tensor will compute all gradients automatically and store them in the `grad` attribute.\n",
    "\n",
    "Let's start by setting the `requires_grad` attribute to `False` and observe the value of `*.grad_fn` and `*.grad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient, requires_grad = False example\n",
    "\n",
    "x = torch.ones(1, requires_grad=False)\n",
    "print(\"Tensor created with default value where requires_grad=False\")\n",
    "print(x, x.dtype, x.requires_grad, x.device)\n",
    "\n",
    "# perform math operation\n",
    "out = x.pow(2)\n",
    "print(\"* grad_fn wrt x:\", x.grad_fn)\n",
    "print(\"* grad wrt x:\", x.grad)\n",
    "print(\"* grad_fn wrt out:\", out.grad_fn)\n",
    "print(\"* grad wrt out:\", out.grad)\n",
    "\n",
    "# This will give error because its requires_grad is set to False. Thus, it has no grad_fn for backprop\n",
    "#out.backward()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, set `requires_grad` to `True`. Math operations on the tensor will be tracked and gradients will be calculated once the `.backward` function is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient,, requires_grad = True\n",
    "# example with multidimensional tensor\n",
    "\n",
    "x = torch.ones(3, 1, requires_grad=True)\n",
    "print(\"Tensor created with requires_grad=True\")\n",
    "print(x, x.dtype, x.requires_grad, x.device)\n",
    "\n",
    "out = x.pow(2).sum()\n",
    "print(\"Output value:\", out, \"size:\", out.size())\n",
    "print(\"* grad_fn wrt x:\", x.grad_fn)\n",
    "print(\"* grad wrt x:\", x.grad)\n",
    "print(\"* grad_fn wrt out:\", out.grad_fn)\n",
    "print(\"* grad wrt out:\", out.grad)\n",
    "\n",
    "print(\"\\nCall .backward function for gradients calcuations:\")\n",
    "out.backward()\n",
    "print(\"* grad_fn wrt x:\", x.grad_fn)\n",
    "print(\"* grad wrt x:\", x.grad)\n",
    "print(\"* grad_fn wrt out:\", out.grad_fn)\n",
    "print(\"* grad wrt out:\", out.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumPy ndarray and Tensor\n",
    "\n",
    "Tensor can be converted to NumPy ndarray and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy ndarray to tensor and vise versa\n",
    "\n",
    "import numpy as np\n",
    "np_v = np.ones(5)\n",
    "\n",
    "# create tensor from numpy\n",
    "tensor_v = torch.from_numpy(np_v)\n",
    "print(\"np_v:\", np_v)\n",
    "print(\"tensor_v:\", tensor_v)\n",
    "\n",
    "# convert tensor to numpy\n",
    "x = tensor_v.numpy()\n",
    "print(\"x:\", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning Training Examples\n",
    "\n",
    "The majority of practical machine learning uses supervised learning. Supervised learning is used to learn the relationships (mapping function) from existing input and output data.  The goal is to be able to use the learned relationships to predict the output data from new input data.\n",
    "\n",
    "### Model Training Flow\n",
    "\n",
    "Training a model involves looping through several fundamental steps:\n",
    "\n",
    "* Define model.\n",
    "* Prepare input and target (label) data in a format that can be consumed by the model.\n",
    "* Run the data through the computations defined by the model.\n",
    "* Get the prediction (output).\n",
    "* Compute loss by comparing prediction to target.\n",
    "* Minimize loss by using an optimization algorithm to adjust the learned variables (weights, biases, ...)\n",
    "![](img/trainingFlow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Linear Regression\n",
    "\n",
    "Simple linear regression is a statistical method that allows us to summarize and study relationships between two continuous (quantitative) variables:\n",
    "\n",
    "* One variable, denoted x, is regarded as the predictor, explanatory, or independent variable.\n",
    "* The other variable, denoted y, is regarded as the response, outcome, or dependent variable.\n",
    "\n",
    "Linear regression uses a linear equation of the form:\n",
    "$\n",
    "   Y = WX + b\n",
    "$\n",
    "\n",
    "Where:\n",
    "* **X**: input data\n",
    "* **Y**: predicted data\n",
    "* **W**: weight to be learned during training\n",
    "* **b**: bias to be learned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set the `device` variable to determine whether our example will be run on CPU or GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "#device = torch.device('cuda') # Uncomment this to run on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, in our linear regression example below, we will ignore the bias term.  So the relationship between our data (X, Y) is just Y = WX. We will try to determine (learn) the value of W.\n",
    "\n",
    "We will create our data by letting X be a tensor with random values and Y is just double of X.\n",
    "\n",
    "To begin with, all training math operations will be performed manually. These include: gradient and loss calculations, weight adjustment, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "import torch\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# D_out is output dimension.\n",
    "N, D_in, D_out = 64, 1, 1\n",
    "\n",
    "# Prepare sample data\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = 2*x\n",
    "\n",
    "# Randomly initialize weights\n",
    "w = torch.randn(D_in, D_out, device=device)\n",
    "print(w, w.size())\n",
    "\n",
    "learning_rate = 1e-4\n",
    "for t in range(1000):    \n",
    "    # Forward pass: compute predicted y\n",
    "    y_pred = x.mm(w)\n",
    "    \n",
    "    # Compute and print loss; loss is a scalar, and is stored in a PyTorch Tensor\n",
    "    # of shape (); we can get its value as a Python number with loss.item()\n",
    "    loss=(y_pred - y).pow(2).sum()\n",
    "    if (t%100 ==0):\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Backprop to compute gradients of with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w = x.t().mm(grad_y_pred)\n",
    "  \n",
    "    # Update weights using gradient descent\n",
    "    w -= learning_rate * grad_w\n",
    "      \n",
    "print(w)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although gradients can be calculated manually, the operations are tedious and error-prone, especially with complex neural networks. In the following example, we will replace the loss calculations with a PyTorch pre-defined loss function. We will also use autograd for gradient computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression using autograd, pre-defined loss function\n",
    "import torch\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# D_out is output dimension.\n",
    "N, D_in, D_out = 64, 1, 1\n",
    "\n",
    "# Prepare data\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = 2*x\n",
    "\n",
    "# Randomly initialize weights, enable autograd\n",
    "w = torch.randn(D_in, D_out, device=device, requires_grad=True)\n",
    "print(w, w.size())\n",
    "\n",
    "# Use PyTorch pre-defined loss function\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')  # for PyTorch 0.4.1 and later\n",
    "#loss_fn = torch.nn.MSELoss(size_average=False) # for PyTorch 0.4.0\n",
    "\n",
    "learning_rate = 1e-4\n",
    "for t in range(1000):\n",
    "    \n",
    "    # Forward pass: compute predicted y\n",
    "    y_pred = x.mm(w)\n",
    "\n",
    "    # Compute and print loss; loss is a scalar, and is stored in a PyTorch Tensor\n",
    "    # of shape (); we can get its value as a Python number with loss.item().\n",
    "    loss = loss_fn(y_pred, y)  \n",
    "    if (t%100 ==0):\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # call backward to compute gradients of w with respect to loss        \n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Update weights using gradient descent\n",
    "        w -= learning_rate * w.grad\n",
    "    w.grad.zero_()\n",
    "\n",
    "print(w)\n",
    "x = torch.randn(1, D_in, device=device)\n",
    "print(x)\n",
    "print(x.mm(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch includes a number of optimization algorithms for trainable parameter adjustment. In the next example, we will use one of those optimizers and calling its `.step()` function to adjust the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression using PyTorch optimizer functions\n",
    "import torch\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# D_out is output dimension.\n",
    "N, D_in, D_out = 64, 1, 1\n",
    "\n",
    "# Prepare data\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = 2*x\n",
    "\n",
    "# Randomly initialize weights, enable autograd\n",
    "w = torch.randn(D_in, D_out, device=device, requires_grad=True)\n",
    "print(w, w.size())\n",
    "\n",
    "# Use PyTorch pre-defined loss function\n",
    "# loss_fn = torch.nn.MSELoss(reduction='sum')  # for PyTorch 0.4.1\n",
    "loss_fn = torch.nn.MSELoss(size_average=False) # for PyTorch 0.4.0\n",
    "\n",
    "learning_rate = 1e-4\n",
    "# Use PyTorch pre-defined optimizer\n",
    "optimizer = torch.optim.SGD([w], lr=learning_rate)\n",
    "\n",
    "for t in range(1000):\n",
    "    \n",
    "    # Forward pass: compute predicted y\n",
    "    y_pred = x.mm(w)\n",
    "\n",
    "    # Compute and print loss; loss is a scalar, and is stored in a PyTorch Tensor\n",
    "    # of shape (); we can get its value as a Python number with loss.item().\n",
    "    loss = loss_fn(y_pred, y)  \n",
    "    if (t%100 ==0):\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "print(w)\n",
    "x = torch.randn(1, D_in, device=device)\n",
    "print(x)\n",
    "print(x.mm(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch `nn` package defines a set of Modules, which you can think of as a neural network layer that produces output from input and may have some trainable weights. In the following example, we will use `torch.nn.Linear` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression using torch.nn.model\n",
    "import torch\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# D_out is output dimension.\n",
    "N, D_in, D_out = 64, 1, 1\n",
    "\n",
    "# Prepare data\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = 2*x\n",
    "\n",
    "# Use PyTorch pre-defined loss function\n",
    "# loss_fn = torch.nn.MSELoss(reduction='sum')  # for PyTorch 0.4.1\n",
    "loss_fn = torch.nn.MSELoss(size_average=False) # for PyTorch 0.4.0\n",
    "\n",
    "# Linear model\n",
    "model=torch.nn.Linear(D_in, D_out, bias=False).to(device)\n",
    "\n",
    "learning_rate = 1e-4\n",
    "# Use PyTorch pre-defined optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for t in range(1000):\n",
    "    \n",
    "    # Forward pass: compute predicted y\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss; loss is a scalar, and is stored in a PyTorch Tensor\n",
    "    # of shape (); we can get its value as a Python number with loss.item().\n",
    "    loss = loss_fn(y_pred, y)  \n",
    "    if (t%100 ==0):\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "for p in model.parameters():\n",
    "    print(\"parameter:\",p.data.size(), p.data)\n",
    "       \n",
    "x = torch.randn(2, D_in, device=device)\n",
    "print('Input', x)\n",
    "print('Predict', model.forward(x))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Network\n",
    "\n",
    "A multilayer network is a stacked representation of a single-layer neural network. The input layer is tacked onto the first-layer neural network and a feed-forward network. Each subsequent layer after the input layer uses the output of the previous layer as its input.\n",
    "\n",
    "In the following example, we replace the simple regression model with a fully-connected ReLU network from https://github.com/jcjohnson/pytorch-examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multilayer model\n",
    "import torch\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "          torch.nn.Linear(D_in, H),\n",
    "          torch.nn.ReLU(),\n",
    "          torch.nn.Linear(H, D_out),\n",
    "        ).to(device)\n",
    "\n",
    "# Use PyTorch pre-defined loss function\n",
    "# loss_fn = torch.nn.MSELoss(reduction='sum')  # for PyTorch 0.4.1\n",
    "loss_fn = torch.nn.MSELoss(size_average=False) # for PyTorch 0.4.0\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    y_pred = model(x) \n",
    "\n",
    "    # Compute and print loss; loss is a scalar, and is stored in a PyTorch Tensor\n",
    "    # of shape (); we can get its value as a Python number with loss.item().\n",
    "    loss = loss_fn(y_pred, y)  \n",
    "    if (t%100 == 0):\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "for p in model.parameters():\n",
    "    print(\"parameter:\",p.data.size(), p.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example ( from https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html) shows how to build\n",
    "a customized model for multilayer networks that can't be built with a sequence of existing Modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom nn modules\n",
    "import torch\n",
    "\n",
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
    "        member variables.\n",
    "        \"\"\"\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary (differentiable) operations on Tensors.\n",
    "        \"\"\"\n",
    "        h_relu = self.linear1(x).clamp(min=0)\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "device = torch.device('cpu')\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "# Construct our model by instantiating the class defined above.\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "model.to(device)\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "# loss_fn = torch.nn.MSELoss(reduction='sum')  # for PyTorch 0.4.1\n",
    "loss_fn = torch.nn.MSELoss(size_average=False) # for PyTorch 0.4.0\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "for t in range(1000):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if (t%100 == 0):\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* https://github.com/jcjohnson/pytorch-examples\n",
    "* https://machinelearningmastery.com/linear-regression-for-machine-learning/\n",
    "* https://machinelearningmastery.com/supervised-and-unsupervised-machine-learning-algorithms/\n",
    "* https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.htm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
