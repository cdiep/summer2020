{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face recognizer using pre-trained VGG-Face model\n",
    "\n",
    "This note book is inspired by Sefik Ilkin Serengil's [Deep Face Recognition with Keras](https://sefiks.com/2018/08/06/deep-face-recognition-with-keras/) implementation. His code can be found at \n",
    "https://github.com/serengil/tensorflow-101/blob/master/python/vgg-face.ipynb\n",
    "\n",
    "Sefik's original notebook was updated to use the tf.keras submodule instead of the standalone keras pacakage.\n",
    "tf.keras was introduced in TensorFlow v1.10.0, this is the first step in integrating Keras directly within the\n",
    "TensorFlow package itself. \n",
    "\n",
    "The [Deep Face Recognition](http://www.robots.ox.ac.uk/~vgg/publications/2015/Parkhi15/parkhi15.pdf) VGG-Face CNN architecture along with its pre-trained weights were published by the Visual Geometry Group of University of Oxford [here](https://www.robots.ox.ac.uk/~vgg/software/vgg_face/). This notebook will use the pre-trained VGG-Face model to recognize whether the two pictures of persons presented are the same or different person. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the prerequisite libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Convolution2D, ZeroPadding2D, MaxPooling2D, Flatten, Dense, Dropout, Activation\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import load_img, save_img, img_to_array\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contruct the neural network model\n",
    "\n",
    "VGG-Face has 22 layers and 37 deep units. Here is how Sefik visualized the VGG-Face architure.\n",
    "\n",
    "![](img/vgg-face.JPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(ZeroPadding2D((1,1),input_shape=(224,224, 3)))\n",
    "model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model.add(Convolution2D(4096, (7, 7), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Convolution2D(4096, (1, 1), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Convolution2D(2622, (1, 1)))\n",
    "model.add(Flatten())\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model with pretrained weights\n",
    "\n",
    "Although the pre-trained weights can be downloaded from the Visual Geometry Group's [Website](https://www.robots.ox.ac.uk/~vgg/software/vgg_face/), but it is matlab compatible. \n",
    "The transformed pre-trained weights for Keras can be downloaded from this link \n",
    "https://drive.google.com/file/d/1CPSeum3HpopfomUEK1gybeuIVoeJT_Eo/view?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('vgg_face_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing image\n",
    "\n",
    "Images are preprocessed to transform them into numeric representation vectors of sizes that are expected by the VGG-Face model.\n",
    "Notice that VGG model expects 224x224x3 sized input images. The 3rd dimension refers to number of channels or RGB colors. Besides, preprocess_input function normalizes input in scale of [-1, +1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    img = load_img(image_path, target_size=(224, 224))\n",
    "    img = img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = preprocess_input(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "picture = \"test-images/jolie1-c.jpg\"\n",
    "img = preprocess_image(picture)\n",
    "print(\"Image shape:\", img.shape)\n",
    "print(img)\n",
    "\n",
    "plt.imshow(image.load_img((picture)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model for similarity determination\n",
    "\n",
    "Create a new face descriptor  model by omitting the last layer of the VGG-Face model. The output vectors of this model will be used as the face descriptor \n",
    "representations for similarity determination. \n",
    "\n",
    "Vectors similarity is measured by their distance. There are two common ways to find the distance of two vectors: cosine distance and euclidean distance. Cosine distance is\n",
    "equal to 1 minus cosine similarity. No matter which measurement we adapt, they all serve for finding similarities between vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_face_descriptor = Model(inputs=model.layers[0].input, outputs=model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findCosineSimilarity(source_representation, test_representation):\n",
    "    a = np.matmul(np.transpose(source_representation), test_representation)\n",
    "    b = np.sum(np.multiply(source_representation, source_representation))\n",
    "    c = np.sum(np.multiply(test_representation, test_representation))\n",
    "    return 1 - (a / (np.sqrt(b) * np.sqrt(c)))\n",
    "\n",
    "def findEuclideanDistance(source_representation, test_representation):\n",
    "    euclidean_distance = source_representation - test_representation\n",
    "    euclidean_distance = np.sum(np.multiply(euclidean_distance, euclidean_distance))\n",
    "    euclidean_distance = np.sqrt(euclidean_distance)\n",
    "    return euclidean_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify face similarity\n",
    "\n",
    "This function calculate similarity between the face images by passing the numerical representations of images to the neural \n",
    "network. The outputs are the \"face descriptor\" representation vectors. These vectors are then be used to compared with\n",
    "each other by calculating cosine distance and/or euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.40\n",
    "\n",
    "def verifyFace(img1, img2):\n",
    "    img1_representation = vgg_face_descriptor.predict(preprocess_image((img1)))[0,:]\n",
    "    img2_representation = vgg_face_descriptor.predict(preprocess_image((img2)))[0,:]\n",
    "    \n",
    "    cosine_similarity = findCosineSimilarity(img1_representation, img2_representation)\n",
    "    euclidean_distance = findEuclideanDistance(img1_representation, img2_representation)\n",
    "    \n",
    "    print(\"Cosine similarity: \",cosine_similarity)\n",
    "    print(\"Euclidean distance: \",euclidean_distance)\n",
    "    \n",
    "    if(cosine_similarity < epsilon):\n",
    "        print(\"Persons in \"+ img1 + \" and \" + img2 + \" are the same person\")\n",
    "    else:\n",
    "        print(\"Persons in \"+ img1 + \" and \" + img2 + \" are not the same person\")\n",
    "    \n",
    "    f = plt.figure()\n",
    "    f.add_subplot(1,2, 1)\n",
    "    plt.imshow(image.load_img((img1)))\n",
    "    plt.xticks([]); plt.yticks([])\n",
    "    f.add_subplot(1,2, 2)\n",
    "    plt.imshow(image.load_img((img2)))\n",
    "    plt.xticks([]); plt.yticks([])\n",
    "    plt.show(block=True)\n",
    "    print(\"-----------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare images\n",
    "\n",
    "Finally, let's comnpare some iamges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verifyFace(\"test-images/jolie1-c.jpg\", \"test-images/jolie2-c.jpg\")\n",
    "verifyFace(\"test-images/jolie1-c.jpg\", \"test-images/jolie4-c.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def list_image_files(directory='.'):\n",
    "    extensions = ['.jpg', '.JPG', '.png', '.PNG']\n",
    "    filenames = []\n",
    "    for f in os.listdir(directory):\n",
    "        name, ext = os.path.splitext(f)\n",
    "        if ext in extensions:\n",
    "            filenames.append(f)\n",
    "    return filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some fun activities\n",
    "\n",
    "Upload your own images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import FileUpload, Output, widgets\n",
    "import os\n",
    "\n",
    "image_dir = 'test-img'\n",
    "out = Output()\n",
    "\n",
    "@out.capture()\n",
    "def show_content(change):\n",
    "    for k in change['new'].keys():\n",
    "        print(\"Uploaded file:\", k)\n",
    "    with open(os.path.join(k),\"w+b\") as i:\n",
    "        i.write(change['new'][k]['content'])\n",
    "\n",
    "w = FileUpload(multiple=False)\n",
    "w.observe(show_content, 'value')\n",
    "\n",
    "with out:\n",
    "    display(w)\n",
    "\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare your images\n",
    "\n",
    "Does this face recognition program differentiate people correctly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from ipywidgets import interactive\n",
    "\n",
    "def select_file(file=list_image_files()):\n",
    "    return file\n",
    "\n",
    "file1 = interactive(select_file)\n",
    "file2 = interactive(select_file)\n",
    "\n",
    "button = widgets.Button(description=\"Compare images!\")\n",
    "output = widgets.Output()\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    with output:\n",
    "        verifyFace(file1.result, file2.result)\n",
    "        \n",
    "button.on_click(on_button_clicked)        \n",
    "\n",
    "print(\"Select two image files to compare:\")\n",
    "box = widgets.VBox([file1, file2, button, output])\n",
    "box"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
