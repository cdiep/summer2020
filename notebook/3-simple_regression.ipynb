{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning Training Examples\n",
    "\n",
    "The majority of practical machine learning uses supervised learning. Supervised learning is used to learn the relationships (mapping function) from existing input and output data.  The goal is to be able to use the learned relationships to predict the output data from new input data.\n",
    "\n",
    "### Model Training Flow\n",
    "\n",
    "Training a model involves looping through several fundamental steps:\n",
    "\n",
    "* Define model.\n",
    "* Prepare input and target (label) data in a format that can be consumed by the model.\n",
    "* Run the data through the computations defined by the model.\n",
    "* Get the prediction (output).\n",
    "* Compute loss by comparing prediction to target.\n",
    "* Minimize loss by using an optimization algorithm to adjust the learned variables (weights, biases, ...)\n",
    "![](img/trainingFlow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Linear Regression\n",
    "\n",
    "Simple linear regression is a statistical method that allows us to summarize and study relationships between two continuous (quantitative) variables:\n",
    "\n",
    "* One variable, denoted x, is regarded as the predictor, explanatory, or independent variable.\n",
    "* The other variable, denoted y, is regarded as the response, outcome, or dependent variable.\n",
    "\n",
    "Linear regression uses a linear equation of the form:\n",
    "$\n",
    "   Y = WX + b\n",
    "$\n",
    "\n",
    "Where:\n",
    "* **X**: input data\n",
    "* **Y**: predicted data\n",
    "* **W**: weight to be learned during training\n",
    "* **b**: bias to be learned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, in our linear regression example below, we will ignore the bias term.  So the relationship between our data (X, Y) is just Y = WX. We will try to determine (learn) the value of W.\n",
    "\n",
    "We will create our data by letting X be a tensor with random values and Y is just double of X.\n",
    "\n",
    "To begin with, all training math operations will be performed manually. These include: gradient and loss calculations, weight adjustment, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "import torch\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# D_out is output dimension.\n",
    "N, D_in, D_out = 64, 1, 1\n",
    "\n",
    "# Prepare sample data\n",
    "x = torch.randn(N, D_in)\n",
    "y = 1.1*x\n",
    "\n",
    "# Randomly initialize weights\n",
    "w = torch.randn(D_in, D_out)\n",
    "print(\"Before learning w=\", w, w.size())\n",
    "\n",
    "learning_rate = 1e-4\n",
    "for t in range(1000):    \n",
    "    # Forward pass: compute predicted y\n",
    "    y_pred = x.mm(w)\n",
    "    \n",
    "    # Compute and print loss; loss is a scalar, and is stored in a PyTorch Tensor\n",
    "    # of shape (); we can get its value as a Python number with loss.item()\n",
    "    loss=(y_pred - y).pow(2).sum()\n",
    "    if (t%100 ==0):\n",
    "        print(t, \" loss=\",loss.item(), \" weight=\", w.item())\n",
    "\n",
    "    # Backprop to compute gradients of with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w = x.t().mm(grad_y_pred)\n",
    "  \n",
    "    # Update weights using gradient descent\n",
    "    w -= learning_rate * grad_w\n",
    "      \n",
    "print(\"After learning weight=\", w)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Linear Regression using PyTorch\n",
    "\n",
    "Next, we will use PyTorch provides functions to replace all training math operations that were performed manually in the previous example.\n",
    "\n",
    "* *Define Model*\n",
    "\n",
    "  Pytorch `nn` package defines a set of Modules, which you can think of as a neural network layer that produces output from input and may have some trainable weights. In the following example, we will use the `torch.nn.Linear` module.\n",
    "\n",
    "\n",
    "* *Calculate loss*\n",
    "\n",
    "   Although gradients of the loss function can be calculated manually, the operations are tedious and error-prone,   especially with complex neural networks. In the following example, we will replace the loss calculations with a  PyTorch pre-defined loss function.\n",
    "\n",
    "\n",
    "* *Adjust learning variable(s)*\n",
    "\n",
    "  PyTorch includes a number of optimization algorithms for trainable parameter adjustment. In the next example, we will use one of those optimizers and calling its .step() function to adjust the weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression using torch.nn.model\n",
    "import torch\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# D_out is output dimension.\n",
    "N, D_in, D_out = 64, 1, 1\n",
    "\n",
    "# Prepare data\n",
    "x = torch.randn(N, D_in)\n",
    "y = 2*x\n",
    "\n",
    "# Use PyTorch pre-defined loss function\n",
    "# loss_fn = torch.nn.MSELoss(reduction='sum')  # for PyTorch 0.4.1\n",
    "loss_fn = torch.nn.MSELoss(size_average=False) # for PyTorch 0.4.0\n",
    "\n",
    "# Linear model\n",
    "model=torch.nn.Linear(D_in, D_out, bias=False)\n",
    "\n",
    "w = []\n",
    "for p in model.parameters():\n",
    "    w.append(p.data)\n",
    "print(\"Before learning w=\", w)\n",
    "\n",
    "learning_rate = 1e-4\n",
    "# Use PyTorch pre-defined optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for t in range(1000):\n",
    "    \n",
    "    # Forward pass: compute predicted y\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    # Compute and print loss; loss is a scalar, and is stored in a PyTorch Tensor\n",
    "    # of shape (); we can get its value as a Python number with loss.item().\n",
    "    loss = loss_fn(y_pred, y)  \n",
    "    \n",
    "    w = []\n",
    "    if (t%100 ==0):\n",
    "        #w = []\n",
    "        for p in model.parameters():\n",
    "            w.append(p.data)\n",
    "        print(t, \" loss=\",loss.item(), \" weight=\", w)\n",
    " \n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "for p in model.parameters():\n",
    "    print(\"parameter:\",p.data.size(), p.data)\n",
    "       \n",
    "x = torch.randn(2, D_in)\n",
    "print('Input', x)\n",
    "print('Predict', model.forward(x))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* https://github.com/jcjohnson/pytorch-examples\n",
    "* https://machinelearningmastery.com/linear-regression-for-machine-learning/\n",
    "* https://machinelearningmastery.com/supervised-and-unsupervised-machine-learning-algorithms/\n",
    "* https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.htm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
