{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face recognizer using pre-trained VGG-Face model\n",
    "\n",
    "This note book is inspired by Sefik Ilkin Serengil's [Deep Face Recognition with Keras](https://sefiks.com/2018/08/06/deep-face-recognition-with-keras/) implementation. His code can be found at \n",
    "https://github.com/serengil/tensorflow-101/blob/master/python/vgg-face.ipynb\n",
    "\n",
    "Sefik's original notebook was updated to use the tf.keras submodule instead of the standalone keras pacakage.\n",
    "tf.keras was introduced in TensorFlow v1.10.0, this is the first step in integrating Keras directly within the\n",
    "TensorFlow package itself. \n",
    "\n",
    "The [Deep Face Recognition](http://www.robots.ox.ac.uk/~vgg/publications/2015/Parkhi15/parkhi15.pdf) VGG-Face CNN architecture along with its pre-trained weights were published by the Visual Geometry Group of University of Oxford [here](https://www.robots.ox.ac.uk/~vgg/software/vgg_face/). This notebook will use the pre-trained VGG-Face model to recognize whether the two pictures of persons presented are the same or different person. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup to access data from IBM Object Storage\n",
    "\n",
    "The following file will be downloaded from IBM Object Storage:\n",
    "* vgg_face_weights.h5\n",
    "* jolie1-c.jpg\n",
    "* jolie3-c.jpg\n",
    "* jolie3-c.jpg\n",
    "* jolie4-c.jpg\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the command if ibm_boto3 is not installed.\n",
    "!pip install ibm-cos-sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the boto library.\n",
    "import ibm_boto3\n",
    "from ibm_botocore.client import Config\n",
    "\n",
    "cos_credentials = {\n",
    "  \"apikey\": \"bzaovcHjJb4vRir7CzWw2zopFAKVACb21WKlvSp2EeEv\",\n",
    "  \"endpoints\": \"https://control.cloud-object-storage.cloud.ibm.com/v2/endpoints\",\n",
    "  \"iam_apikey_description\": \"Auto-generated for key 155a3956-0e11-4df1-9228-f00ff0090ea6\",\n",
    "  \"iam_apikey_name\": \"vggh5\",\n",
    "  \"iam_role_crn\": \"crn:v1:bluemix:public:iam::::serviceRole:Writer\",\n",
    "  \"iam_serviceid_crn\": \"crn:v1:bluemix:public:iam-identity::a/d17d7328be3c182a41e9856d62e02770::serviceid:ServiceId-974be549-3a00-4101-80aa-8414ef94a269\",\n",
    "  \"resource_instance_id\": \"crn:v1:bluemix:public:cloud-object-storage:global:a/d17d7328be3c182a41e9856d62e02770:21403250-3385-472c-bc43-97fc64697887::\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define endpoint information.\n",
    "# The service_endpoint can be copied from IBM Cloud as following:\n",
    "# * From IBM Cloud Resource list ( https://cloud.ibm.com/resources ), click on the target storage service.\n",
    "# * Click on the target bucket, then click on the Configuration tab on the left menu.\n",
    "# * Copy the string displace under the public endpoint\n",
    "\n",
    "service_endpoint = 'https://s3.us-east.cloud-object-storage.appdomain.cloud'\n",
    "\n",
    "# Define the authorization endpoint.\n",
    "auth_endpoint = 'https://iam.bluemix.net/oidc/token'\n",
    "\n",
    "# Create a COS resource.\n",
    "cos = ibm_boto3.resource('s3',\n",
    "                         ibm_api_key_id=cos_credentials['apikey'],\n",
    "                         ibm_service_instance_id=cos_credentials['resource_instance_id'],\n",
    "                         ibm_auth_endpoint=auth_endpoint,\n",
    "                         config=Config(signature_version='oauth'),\n",
    "                         endpoint_url=service_endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def download_file_from_cos(bucket_obj, source_filenames, dest_filenames):\n",
    "    for i in range(len(source_filenames)):\n",
    "        if not os.path.isfile(dest_filenames[i]):\n",
    "            print(\"Downloading\", source_filenames[i])\n",
    "            bucket_obj.download_file(source_filenames[i],dest_filenames[i])\n",
    "        else:\n",
    "            print(source_filenames[i], \"was downloaded previously.\" )\n",
    "\n",
    "# The bucket object that has the h5 file.\n",
    "bucket_obj = cos.Bucket('vggh5')\n",
    "            \n",
    "# Files to download from object storage\n",
    "source_filenames = [\"vgg_face_weights.h5\", 'jolie1-c.jpg', 'jolie2-c.jpg', 'jolie3-c.jpg', 'jolie4-c.jpg']\n",
    "dest_filenames = source_filenames\n",
    "\n",
    "download_file_from_cos(bucket_obj, source_filenames, dest_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the prerequisite libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Convolution2D, ZeroPadding2D, MaxPooling2D, Flatten, Dense, Dropout, Activation\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import load_img, save_img, img_to_array\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contruct the neural network model\n",
    "\n",
    "VGG-Face has 22 layers and 37 deep units. Here is how Sefik visualized the VGG-Face architure.\n",
    "\n",
    "![](img/vgg-face.JPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(ZeroPadding2D((1,1),input_shape=(224,224, 3)))\n",
    "model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model.add(Convolution2D(4096, (7, 7), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Convolution2D(4096, (1, 1), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Convolution2D(2622, (1, 1)))\n",
    "model.add(Flatten())\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model with pretrained weights\n",
    "\n",
    "Although the pre-trained weights can be downloaded from the Visual Geometry Group's [Website](https://www.robots.ox.ac.uk/~vgg/software/vgg_face/), but it is matlab compatible. \n",
    "The transformed pre-trained weights for Keras can be downloaded from this link \n",
    "https://drive.google.com/file/d/1CPSeum3HpopfomUEK1gybeuIVoeJT_Eo/view?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('vgg_face_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing image\n",
    "\n",
    "Images are preprocessed to transform them into numeric representation vectors of sizes that are expected by the VGG-Face model.\n",
    "Notice that VGG model expects 224x224x3 sized input images. The 3rd dimension refers to number of channels or RGB colors. Besides, preprocess_input function normalizes input in scale of [-1, +1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    img = load_img(image_path, target_size=(224, 224))\n",
    "    img = img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = preprocess_input(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "picture = \"jolie1-c.jpg\"\n",
    "img = preprocess_image(picture)\n",
    "print(\"Image shape:\", img.shape)\n",
    "print(img)\n",
    "\n",
    "plt.imshow(image.load_img((picture)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model for similarity determination\n",
    "\n",
    "Create a new face descriptor  model by omitting the last layer of the VGG-Face model. The output vectors of this model will be used as the face descriptor \n",
    "representations for similarity determination. \n",
    "\n",
    "Vectors similarity is measured by their distance. There are two common ways to find the distance of two vectors: cosine distance and euclidean distance. Cosine distance is\n",
    "equal to 1 minus cosine similarity. No matter which measurement we adapt, they all serve for finding similarities between vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_face_descriptor = Model(inputs=model.layers[0].input, outputs=model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findCosineSimilarity(source_representation, test_representation):\n",
    "    a = np.matmul(np.transpose(source_representation), test_representation)\n",
    "    b = np.sum(np.multiply(source_representation, source_representation))\n",
    "    c = np.sum(np.multiply(test_representation, test_representation))\n",
    "    return 1 - (a / (np.sqrt(b) * np.sqrt(c)))\n",
    "\n",
    "def findEuclideanDistance(source_representation, test_representation):\n",
    "    euclidean_distance = source_representation - test_representation\n",
    "    euclidean_distance = np.sum(np.multiply(euclidean_distance, euclidean_distance))\n",
    "    euclidean_distance = np.sqrt(euclidean_distance)\n",
    "    return euclidean_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify face similarity\n",
    "\n",
    "This function calculate similarity between the face images by passing the numerical representations of images to the neural \n",
    "network. The outputs are the \"face descriptor\" representation vectors. These vectors are then be used to compared with\n",
    "each other by calculating cosine distance and/or euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.40\n",
    "\n",
    "def verifyFace(img1, img2):\n",
    "    img1_representation = vgg_face_descriptor.predict(preprocess_image((img1)))[0,:]\n",
    "    img2_representation = vgg_face_descriptor.predict(preprocess_image((img2)))[0,:]\n",
    "    \n",
    "    cosine_similarity = findCosineSimilarity(img1_representation, img2_representation)\n",
    "    euclidean_distance = findEuclideanDistance(img1_representation, img2_representation)\n",
    "    \n",
    "    print(\"Cosine similarity: \",cosine_similarity)\n",
    "    print(\"Euclidean distance: \",euclidean_distance)\n",
    "    \n",
    "    if(cosine_similarity < epsilon):\n",
    "        print(\"Persons in \"+ img1 + \" and \" + img2 + \" are the same person\")\n",
    "    else:\n",
    "        print(\"Persons in \"+ img1 + \" and \" + img2 + \" are not the same person\")\n",
    "    \n",
    "    f = plt.figure()\n",
    "    f.add_subplot(1,2, 1)\n",
    "    plt.imshow(image.load_img((img1)))\n",
    "    plt.xticks([]); plt.yticks([])\n",
    "    f.add_subplot(1,2, 2)\n",
    "    plt.imshow(image.load_img((img2)))\n",
    "    plt.xticks([]); plt.yticks([])\n",
    "    plt.show(block=True)\n",
    "    print(\"-----------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare images\n",
    "\n",
    "Finally, let's comnpare some iamges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verifyFace(\"jolie1-c.jpg\", \"jolie2-c.jpg\")\n",
    "verifyFace(\"jolie1-c.jpg\", \"jolie4-c.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare your images\n",
    "\n",
    "Now you can upload your own images to the IBM Cloud Object Storage, download them to this notebook and then compare them.\n",
    "Does this face recognition program differentiate people correctly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comma seperated name string\n",
    "your_image_names=['image1.jpg','image2.jpg']\n",
    "\n",
    "#your_image_names=['jolie3-c','jolie3-c']\n",
    "dest_image_names=your_image_names\n",
    "bucket_obj = cos.Bucket('vggh5')\n",
    "\n",
    "download_file_from_cos(bucket_obj, your_image_names, dest_image_names)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verifyFace(\"image1.jpg\", \"image2.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm *.jpg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
